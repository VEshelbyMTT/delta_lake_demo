{
	"name": "m7_TaxiData_deduplicating_data",
	"properties": {
		"folder": {
			"name": "m7"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "636171fd-54e4-4475-af5c-ab4ab38f7420"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## What this notebook does\r\n",
					"\r\n",
					"Cell one: gets File, paths from ingestion, path to save + loads as df in spark\r\n",
					"\r\n",
					"Cell two: data transformation of the data cell further\r\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"import uuid \r\n",
					"from datetime import date\r\n",
					"\r\n",
					"default_path = 'm7_running_from_notebook'\r\n",
					"runID = str(uuid.uuid4())\r\n",
					"notebook = 'unknown'\r\n",
					"todays_date = str(date.today())\r\n",
					"\r\n",
					"\r\n",
					"print(runID)"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false,
					"tags": []
				},
				"source": [
					"\r\n",
					"# default table of taking the new dataset\r\n",
					"FILE = 'part-00001-0d8d63a4-9d24-458a-8dac-ead1d500a492-c000.snappy.parquet'\r\n",
					"PATH = f'abfss://scratch@traininglakehouse103.dfs.core.windows.net/m6_output/'\r\n",
					"\r\n",
					"# Loading the data and using the display/limit function to show first 10 rows. \r\n",
					"df = spark.read.load(f'{PATH}{FILE}', format='parquet')\r\n",
					"display(df.limit(10))"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df.columns"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## We want to aggregate this table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": true
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false,
					"tags": []
				},
				"source": [
					"import pyspark.sql.functions as F\r\n",
					"\r\n",
					"df_agg = (\r\n",
					"    df\r\n",
					"    .drop(F.col('storeFwdFlag'))\r\n",
					"    .groupBy(['hack_license','medallion','pickup_datetime','dropoff_longitude','dropoff_latitude'])\r\n",
					"    .agg(\r\n",
					"        F.round(F.sum('fare_amount'),2).alias('fareAmountTotal'),\r\n",
					"        F.sum('surcharge').alias('surchageTotal'),\r\n",
					"        F.round(F.mean('mta_tax'),2).alias('taxTotal'),\r\n",
					"        F.round(F.mean('tip_amount'),2).alias('tipAmountAverage'),\r\n",
					"        F.round(F.sum('total_amount'),2).alias('totalAmount')\r\n",
					"    )\r\n",
					"\r\n",
					")\r\n",
					"\r\n",
					""
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print(f'rows from initial dataset: {df.count()}\\n rows from transformation dataset: {df_agg.count()}')"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"#display(df_agg)"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"DESTINATION_PATH = f'abfss://silver@traininglakehouse103.dfs.core.windows.net/taxi_data/{default_path}/{todays_date}/{notebook}/'\r\n",
					"\r\n",
					"\r\n",
					"(\r\n",
					"    df_agg\r\n",
					"    .write\r\n",
					"    .format(\"delta\")\r\n",
					"    .mode(\"overwrite\")\r\n",
					"    .save(f'{DESTINATION_PATH}/{runID}')\r\n",
					")\r\n",
					"print(f'created delta file at: {DESTINATION_PATH} under file location: {runID}')"
				],
				"execution_count": 7
			}
		]
	}
}