{
	"name": "Explore sample data with Spark",
	"properties": {
		"folder": {
			"name": "m4"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "62e09309-1f71-430d-a9de-449b7f9284d9"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Explore NYC Yellow Taxi Data using Spark"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Load Data\n",
					"\n",
					"Read NYC Yellow Taxi data as a Spark DataFrame object to manipulate."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Read NYC yellow cab data from Azure Open Datasets\n",
					"from azureml.opendatasets import NycTlcYellow\n",
					"\n",
					"from datetime import datetime\n",
					"from dateutil import parser\n",
					"\n",
					"end_date = parser.parse('2018-05-08 00:00:00')\n",
					"start_date = parser.parse('2018-05-01 00:00:00')\n",
					"\n",
					"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
					"df_nyc_tlc = nyc_tlc.to_spark_dataframe()"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## creating a table to join"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"payment_type_data = [(\"1\", \"skittles\"),\r\n",
					"        (\"2\", \"buttons\"),\r\n",
					"        (\"3\", \"high-fives\"),\r\n",
					"        (\"4\", \"castles\" )\r\n",
					"        ]\r\n",
					"columns = [\"paymentType\",\"paymentMethod\"]\r\n",
					"\r\n",
					"payment_df = spark.createDataFrame(payment_type_data, columns)\r\n",
					"    \r\n",
					"payment_df.show()"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Assessing the data\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"df_nyc_tlc.printSchema()"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Customize data visualization with Spark and notebooks\n",
					"You can control how charts render by using notebooks. The following code shows a simple example. It uses the popular libraries matplotlib and seaborn. The code renders the same kind of line chart as the SQL queries we ran earlier.\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import matplotlib.pyplot\n",
					"import seaborn as sns\n",
					"\n",
					"sns.set(style = \"whitegrid\")\n",
					"pdf_nyc = df_nyc.toPandas()\n",
					"#sns.lineplot(x=\"passengerCount\", y=\"SumTripDistance\" , data = pdf_nyc)\n",
					"sns.lineplot(x=\"passengerCount\", y=\"AvgTripDistance\" , data = pdf_nyc)\n",
					"matplotlib.pyplot.show()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [
								"AvgTripDistance"
							],
							"values": [
								"passengerCount"
							],
							"yLabel": "passengerCount",
							"xLabel": "AvgTripDistance",
							"aggregation": "SUM",
							"aggByBackend": false
						},
						"aggData": "{\"passengerCount\":{\"2.382\":7,\"2.9365876998482907\":0,\"2.955385293728598\":1,\"3.0823106614325835\":6,\"3.1096431007047065\":5,\"3.124120509875713\":3,\"3.132080374155551\":4,\"3.1983281312300624\":2,\"6.23\":9,\"7.831666666666666\":8}}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": false
					},
					"collapsed": false
				},
				"source": [
					"from pyspark.sql import functions as F\n",
					"\n",
					"df_nyc = (\n",
					"    df_nyc_tlc\n",
					"    .groupBy([\"passengerCount\", \"paymentType\", \"puYear\"])\n",
					"    .agg(\n",
					"        F.avg('tripDistance').alias('AvgTripDistance'),\n",
					"        F.sum('tripDistance').alias('SumTripDistance')\n",
					"    )\n",
					"    .join(\n",
					"        payment_df,\n",
					"        ['paymentType'],\n",
					"        'left')\n",
					"    .orderBy(\"passengerCount\")\n",
					")\n",
					"display(df_nyc)"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Sharing Data Between Spark And Serverless SQL\r\n",
					"You can transform a spark dataframe to Serverless SQL\r\n",
					"\r\n",
					"Difference: Spark infers your schema and SQL does not and makes assumptions about the data e.g. a String char 2000 "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create a table in my serverless SQL database\r\n",
					"\r\n",
					"spark.sql(\"CREATE DATABASE IF NOT EXISTS NycAgg\")\r\n",
					"(\r\n",
					"    df_nyc\r\n",
					"    .write\r\n",
					"    .mode(\"overwrite\")\r\n",
					"    .saveAsTable(\"NycAgg.PassengerCountYear\")\r\n",
					")"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## .. or Spark to a dedicated SQL pool\r\n",
					"\r\n",
					"write data from a dataframe to a table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"\r\n",
					"#Create a temporary view for the new dataset to load from Scala\r\n",
					"df_nyc.createOrReplaceTempView(\"NycAgg\")\r\n",
					"\r\n",
					""
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "scala"
					}
				},
				"source": [
					"%%spark\r\n",
					"\r\n",
					"val sql_df = spark.sqlContext.sql(\"SELECT * FROM NycAgg\")\r\n",
					"sql_df.write.sqlanalytics(\"SQL_pool_low_performance.dbo.NycAgg\", Constants.INTERNAL)"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Clean up resources\n",
					"To ensure the Spark instance is shut down, end any connected sessions(notebooks). The pool shuts down when the **idle time** specified in the Apache Spark pool is reached. You can also select **stop session** from the status bar at the upper right of the notebook.\n",
					"\n",
					"![stopsession](https://adsnotebookrelease.blob.core.windows.net/adsnotebookrelease/adsnotebook/image/stopsession.png)"
				]
			}
		]
	}
}